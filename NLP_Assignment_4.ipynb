{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Y14ixVqNGol0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentence_transformers in /opt/homebrew/lib/python3.10/site-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /opt/homebrew/lib/python3.10/site-packages (from sentence_transformers) (1.2.1)\n",
            "Requirement already satisfied: tqdm in /opt/homebrew/lib/python3.10/site-packages (from sentence_transformers) (4.64.1)\n",
            "Requirement already satisfied: torchvision in /opt/homebrew/lib/python3.10/site-packages (from sentence_transformers) (0.15.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /opt/homebrew/lib/python3.10/site-packages (from sentence_transformers) (4.26.0)\n",
            "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.10/site-packages (from sentence_transformers) (1.24.2)\n",
            "Requirement already satisfied: scipy in /opt/homebrew/lib/python3.10/site-packages (from sentence_transformers) (1.10.0)\n",
            "Requirement already satisfied: sentencepiece in /opt/homebrew/lib/python3.10/site-packages (from sentence_transformers) (0.1.98)\n",
            "Requirement already satisfied: torch>=1.6.0 in /opt/homebrew/lib/python3.10/site-packages (from sentence_transformers) (2.0.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /opt/homebrew/lib/python3.10/site-packages (from sentence_transformers) (0.12.0)\n",
            "Requirement already satisfied: nltk in /opt/homebrew/lib/python3.10/site-packages (from sentence_transformers) (3.8.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /opt/homebrew/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/homebrew/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.4.0)\n",
            "Requirement already satisfied: requests in /opt/homebrew/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.28.2)\n",
            "Requirement already satisfied: jinja2 in /opt/homebrew/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n",
            "Requirement already satisfied: networkx in /opt/homebrew/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (3.0)\n",
            "Requirement already satisfied: sympy in /opt/homebrew/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (1.11.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /opt/homebrew/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.10.31)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/homebrew/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.13.2)\n",
            "Requirement already satisfied: joblib in /opt/homebrew/lib/python3.10/site-packages (from nltk->sentence_transformers) (1.2.0)\n",
            "Requirement already satisfied: click in /opt/homebrew/lib/python3.10/site-packages (from nltk->sentence_transformers) (8.1.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/homebrew/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/homebrew/lib/python3.10/site-packages (from torchvision->sentence_transformers) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.0.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/homebrew/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.26.14)\n",
            "Requirement already satisfied: mpmath>=0.19 in /opt/homebrew/lib/python3.10/site-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "U8sVGikmGksg"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer, SentencesDataset, InputExample, losses, util, models, evaluation\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "from post_parser_record import PostParserRecord\n",
        "\n",
        "def read_tsv_test_data(file_path):\n",
        "  # Takes in the file path for test file and generate a dictionary\n",
        "  # of question id as the key and the list of question ids similar to it\n",
        "  # as value. It also returns the list of all question ids that have\n",
        "  # at least one similar question\n",
        "  dic_similar_questions = {}\n",
        "  lst_all_test = []\n",
        "  with open(file_path) as fd:\n",
        "    rd = csv.reader(fd, delimiter=\"\\t\", quotechar='\"')\n",
        "    for row in rd:\n",
        "        question_id = int(row[0])\n",
        "        lst_similar = list(map(int, row[1:]))\n",
        "        dic_similar_questions[question_id] = lst_similar\n",
        "        lst_all_test.append(question_id)\n",
        "        lst_all_test.extend(lst_similar)\n",
        "  return dic_similar_questions, lst_all_test\n",
        "\n",
        "dic_similar_questions, lst_all_test = read_tsv_test_data(\"duplicate_questions.tsv\")\n",
        "post_reader = PostParserRecord(\"Posts_law.xml\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDpmkKCXQ-nJ",
        "outputId": "aac02a24-880a-49a9-e766-b935813d44ae"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ef9f2e20db754d8bbbdbab2e50cbdb65",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "85d68664dc864d34a139a481eafc3178",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.02k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b54bfd944b4e4fed9b64d3e924963a38",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/222k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f0138544c5bb4c1c85a3f54d731a1196",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fe45a69dfdcf46b18f2077c0a6e2a5ab",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at nlpaueb/legal-bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0.8943)\n"
          ]
        }
      ],
      "source": [
        "## https://huggingface.co/nlpaueb/legal-bert-base-uncased\n",
        "## https://www.sbert.net/examples/applications/computing-embeddings/README.html?highlight=autotokenizer\n",
        "\n",
        "\n",
        "#Mean Pooling - Take attention mask into account for correct averaging\n",
        "def mean_pooling(model_output, attention_mask):\n",
        "    token_embeddings = model_output[0] #First element of model1_output contains all token embeddings\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
        "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "    return sum_embeddings / sum_mask\n",
        "\n",
        "\n",
        "\n",
        "#Sentences we want sentence embeddings for\n",
        "sentences = ['If a company agrees to pay travel cost for a job interview, is the promise binding and enforceable?',\n",
        "             'Is a job offer letter sent and accepted by email, legally binding?']\n",
        "\n",
        "#Load AutoModel from huggingface model repository\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"nlpaueb/legal-bert-base-uncased\")\n",
        "model = AutoModel.from_pretrained(\"nlpaueb/legal-bert-base-uncased\")\n",
        "\n",
        "#Tokenize sentences\n",
        "encoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
        "\n",
        "#Compute token embeddings\n",
        "with torch.no_grad():\n",
        "    model_output = model(**encoded_input)\n",
        "\n",
        "#Perform pooling. In this case, mean pooling\n",
        "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
        "cos = torch.nn.CosineSimilarity(dim=0)\n",
        "output = cos(sentence_embeddings[0], sentence_embeddings[1])\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ff1894ae0fc349f2ac4dba2701d36211",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/756 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "P@1 ignoring question of itself: 0.156\n",
            "P@1 keep same question: 0.011\n",
            "Mean Reciprocal Rank from 100 queries: 0.095\n"
          ]
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n",
        "\n",
        "# in question one, we are using the pre-trained model on quora with no further fine-tuning\n",
        "model_name = 'distilbert-base-nli-stsb-quora-ranking'\n",
        "model = SentenceTransformer(model_name)\n",
        "\n",
        "# list of text to be indexed (encoded)\n",
        "corpus = []\n",
        "# this dictionary is used as key: corpus index [0, 1, 2, ...] and value: corresponding question id\n",
        "index_to_question_id = {}\n",
        "idx = 0\n",
        "\n",
        "lst_test_question_ids = list(dic_similar_questions.keys())\n",
        "\n",
        "# indexing all the questions in the law stack exchange -- only using the question titles\n",
        "for question_id in post_reader.map_questions:\n",
        "  question = post_reader.map_questions[question_id]\n",
        "  text = question.title\n",
        "  q_id = question.post_id\n",
        "  corpus.append(text)\n",
        "  index_to_question_id[idx] = question_id\n",
        "  idx += 1\n",
        "\n",
        "# Indexing (embedding) the \n",
        "corpus_embeddings = model.encode(corpus, convert_to_tensor=True, show_progress_bar=True)\n",
        "\n",
        "top_k = 100\n",
        "\n",
        "p1 = 0\n",
        "\n",
        "for question_id in lst_test_question_ids:\n",
        "\n",
        "  query_text = post_reader.map_questions[question_id].title\n",
        "  query_embedding = model.encode(query_text, convert_to_tensor=True)\n",
        "\n",
        "  # We use cosine-similarity and torch.topk to find the highest scores\n",
        "  cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]\n",
        "  top_results = torch.topk(cos_scores, k=2)\n",
        "  isPrinted = False;\n",
        "  for score, idx in zip(top_results[0], top_results[1]):\n",
        "    index = int(idx)\n",
        "    # printing question id and similarity score\n",
        "    if question_id != index_to_question_id[index] and not isPrinted:\n",
        "      # print(str(question_id), \"->\", index_to_question_id[index], \"(Score: {:.4f})\".format(score))\n",
        "      if index_to_question_id[index] in dic_similar_questions[question_id]:\n",
        "        p1 = p1 + 1\n",
        "      isPrinted = True\n",
        "\n",
        "print(\"P@1 ignoring question of itself: \" + str(round(p1 / len(lst_test_question_ids), 3)))\n",
        "p1 = 0\n",
        "\n",
        "for question_id in lst_test_question_ids:\n",
        "\n",
        "  query_text = post_reader.map_questions[question_id].title\n",
        "  query_embedding = model.encode(query_text, convert_to_tensor=True)\n",
        "\n",
        "  # We use cosine-similarity and torch.topk to find the highest scores\n",
        "  cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]\n",
        "  top_results = torch.topk(cos_scores, k=1)\n",
        "  isPrinted = False;\n",
        "  for score, idx in zip(top_results[0], top_results[1]):\n",
        "    index = int(idx)\n",
        "    if index_to_question_id[index] in dic_similar_questions[question_id]:\n",
        "      p1 = p1 + 1\n",
        "\n",
        "print(\"P@1 keep same question: \" + str(round(p1 / len(lst_test_question_ids), 3)))\n",
        "\n",
        "mrrSum = 0\n",
        "\n",
        "for question_id in lst_test_question_ids:\n",
        "  query_text = post_reader.map_questions[question_id].title\n",
        "  query_embedding = model.encode(query_text, convert_to_tensor=True)\n",
        "\n",
        "  # We use cosine-similarity and torch.topk to find the highest 5 scores\n",
        "  cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]\n",
        "  top_results = torch.topk(cos_scores, k=top_k)\n",
        "  totalCount = 0\n",
        "  for score, idx in zip(top_results[0], top_results[1]):\n",
        "    totalCount = totalCount + 1\n",
        "    if index_to_question_id[index] in dic_similar_questions[question_id]:\n",
        "      mrrSum = mrrSum + (1 / totalCount)\n",
        "      break\n",
        "    index = int(idx)\n",
        "    # printing question id and similarity score\n",
        "    # print(index_to_question_id[index], \"(Score: {:.4f})\".format(score))\n",
        "print(\"Mean Reciprocal Rank from 100 queries: \" + str(round(mrrSum / len(lst_test_question_ids), 3)))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Used for testing P@1 value and MRR for fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def modelTest(model1, dic_similar_question_test):\n",
        "\n",
        "  # Indexing (embedding) the \n",
        "  corpus_embeddings1 = model1.encode(corpus, convert_to_tensor=True, show_progress_bar=True)\n",
        "\n",
        "  top_k = 100\n",
        "\n",
        "  p1 = 0\n",
        "\n",
        "  lst_test_question_ids = list(dic_similar_question_test.keys())\n",
        "\n",
        "  for question_id in lst_test_question_ids:\n",
        "\n",
        "    query_text = post_reader.map_questions[question_id].title\n",
        "    query_embedding = model1.encode(query_text, convert_to_tensor=True)\n",
        "\n",
        "    # We use cosine-similarity and torch.topk to find the highest scores\n",
        "    cos_scores = util.cos_sim(query_embedding, corpus_embeddings1)[0]\n",
        "    top_results = torch.topk(cos_scores, k=2)\n",
        "    isPrinted = False;\n",
        "    for score, idx in zip(top_results[0], top_results[1]):\n",
        "      index = int(idx)\n",
        "      # printing question id and similarity score\n",
        "      if question_id != index_to_question_id[index] and not isPrinted:\n",
        "        # print(str(question_id), \"->\", index_to_question_id[index], \"(Score: {:.4f})\".format(score))\n",
        "        if index_to_question_id[index] in dic_similar_question_test[question_id]:\n",
        "          p1 = p1 + 1\n",
        "        isPrinted = True\n",
        "\n",
        "  print(\"P@1 ignoring question of itself: \" + str(round(p1 / len(lst_test_question_ids), 4)))\n",
        "  p1 = 0\n",
        "\n",
        "  for question_id in lst_test_question_ids:\n",
        "\n",
        "    query_text = post_reader.map_questions[question_id].title\n",
        "    query_embedding = model1.encode(query_text, convert_to_tensor=True)\n",
        "\n",
        "    # We use cosine-similarity and torch.topk to find the highest scores\n",
        "    cos_scores = util.cos_sim(query_embedding, corpus_embeddings1)[0]\n",
        "    top_results = torch.topk(cos_scores, k=1)\n",
        "    isPrinted = False;\n",
        "    for score, idx in zip(top_results[0], top_results[1]):\n",
        "      index = int(idx)\n",
        "      if index_to_question_id[index] in dic_similar_question_test[question_id]:\n",
        "        p1 = p1 + 1\n",
        "\n",
        "  print(\"P@1 keep same question: \" + str(round(p1 / len(lst_test_question_ids), 4)))\n",
        "\n",
        "  mrrSum = 0\n",
        "  mrrLst = []\n",
        "\n",
        "  for question_id in lst_test_question_ids:\n",
        "    query_text = post_reader.map_questions[question_id].title\n",
        "    query_embedding = model1.encode(query_text, convert_to_tensor=True)\n",
        "\n",
        "    # We use cosine-similarity and torch.topk to find the highest 5 scores\n",
        "    cos_scores = util.cos_sim(query_embedding, corpus_embeddings1)[0]\n",
        "    top_results = torch.topk(cos_scores, k=top_k)\n",
        "    totalCount = 0\n",
        "    found = False\n",
        "    for score, idx in zip(top_results[0], top_results[1]):\n",
        "      totalCount = totalCount + 1\n",
        "      if index_to_question_id[index] in dic_similar_question_test[question_id]:\n",
        "        mrrSum = mrrSum + (1 / totalCount)\n",
        "        mrrLst.append(1 / totalCount)\n",
        "        found = True\n",
        "        break\n",
        "      index = int(idx)\n",
        "    if not found:\n",
        "        mrrLst.append(0)\n",
        "        \n",
        "  print(\"Mean Reciprocal Rank from 100 queries: \" + str(round(mrrSum / len(lst_test_question_ids), 4)))\n",
        "\n",
        "  return mrrLst\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fine-tuning distilbert-base-nli-stsb-quora-ranking with lost function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3015264cfd8b447d901453c5c41012ff",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "527fa3435a744d55bfa4b8c8a7aae0de",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Iteration:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "941f5b6c9a614791a5f2b9b4d3af38ee",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Iteration:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ddfddbf9d636469b8d04029ffdc0d205",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Iteration:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8fd44c4f884d4d019ca41b84b02d2239",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Iteration:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c677abd306914539812058dcccd89fda",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Iteration:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fine-tune model loss function:\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "50cf9dfeee504a14880977bfcb77919e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/756 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "P@1 ignoring question of itself: 0.1417\n",
            "P@1 keep same question: 0.0118\n",
            "Mean Reciprocal Rank from 100 queries: 0.0931\n",
            "Pre-trained model:\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2ed13040fd9f4308be5ab848a88d79bd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/756 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "P@1 ignoring question of itself: 0.1417\n",
            "P@1 keep same question: 0.0118\n",
            "Mean Reciprocal Rank from 100 queries: 0.0925\n",
            "B - A 0.0006575965014532781\n",
            "standard dev: 0.001447521727046236\n",
            "Square root n: 15.937377450509228\n",
            "t: 7.2402\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import random\n",
        "import statistics\n",
        "import math\n",
        "\n",
        "train_samples_MultipleNegativesRankingLoss = []\n",
        "train_batch_size = 128\n",
        "num_epochs = 5\n",
        "model_save_path = \"owenBeanModel.bin\"\n",
        "margin = 2\n",
        "distance_metric = 2\n",
        "\n",
        "model_name = 'distilbert-base-nli-stsb-quora-ranking'\n",
        "model = SentenceTransformer(model_name)\n",
        "\n",
        "count = 0\n",
        "\n",
        "dic_similar_question_test = {}\n",
        "\n",
        "questionMap = list(post_reader.map_questions.keys())\n",
        "\n",
        "for question_id in lst_test_question_ids:\n",
        "\n",
        "  query_text = post_reader.map_questions[question_id].title\n",
        "  query_embedding = model.encode(query_text, convert_to_tensor=True)\n",
        "\n",
        "  if count > len(lst_test_question_ids) * 0.9:\n",
        "    for duplicate_id in dic_similar_questions[question_id]:\n",
        "      train_samples_MultipleNegativesRankingLoss.append(InputExample(texts=[post_reader.map_questions[question_id].title, post_reader.map_questions[duplicate_id].title], label=1))\n",
        "    \n",
        "    index = random.randint(0, len(post_reader.map_questions) - 1)\n",
        "\n",
        "    if questionMap[index] not in dic_similar_questions[question_id]:\n",
        "      train_samples_MultipleNegativesRankingLoss.append(InputExample(texts=[post_reader.map_questions[question_id].title, post_reader.map_questions[questionMap[index]].title], label=0))\n",
        "  else:\n",
        "    dic_similar_question_test[question_id] = dic_similar_questions[question_id]\n",
        "\n",
        "  count = count + 1\n",
        "\n",
        "\n",
        "# Create data loader and loss for MultipleNegativesRankingLoss\n",
        "train_dataset_MultipleNegativesRankingLoss = SentencesDataset(train_samples_MultipleNegativesRankingLoss, model=model)\n",
        "train_dataloader_MultipleNegativesRankingLoss = DataLoader(train_dataset_MultipleNegativesRankingLoss, shuffle=True, batch_size=train_batch_size)\n",
        "train_loss_MultipleNegativesRankingLoss = losses.MultipleNegativesRankingLoss(model)\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_objectives=[(train_dataloader_MultipleNegativesRankingLoss, train_loss_MultipleNegativesRankingLoss)],\n",
        "          epochs=num_epochs,\n",
        "          warmup_steps=1000,\n",
        "          output_path=model_save_path\n",
        "          )\n",
        "\n",
        "print(\"Fine-tune model loss function:\")\n",
        "fine_tune_mrr_list = modelTest(model, dic_similar_question_test)\n",
        "\n",
        "model_name = 'distilbert-base-nli-stsb-quora-ranking'\n",
        "model = SentenceTransformer(model_name)\n",
        "\n",
        "print(\"Pre-trained model:\")\n",
        "pretrained_mrr_list = modelTest(model, dic_similar_question_test)\n",
        "\n",
        "fine_tune_mrr = 0\n",
        "for mrr in fine_tune_mrr_list:\n",
        "  fine_tune_mrr = fine_tune_mrr + mrr\n",
        "fine_tune_mrr = fine_tune_mrr / len(fine_tune_mrr_list)\n",
        "\n",
        "pretrained_mrr = 0\n",
        "for mrr in pretrained_mrr_list:\n",
        "  pretrained_mrr = pretrained_mrr + mrr\n",
        "pretrained_mrr = pretrained_mrr / len(pretrained_mrr_list)\n",
        "\n",
        "\n",
        "fine_tune_deviation = statistics.stdev(fine_tune_mrr_list)\n",
        "pretrained_deviation = statistics.stdev(pretrained_mrr_list)\n",
        "\n",
        "b_a = fine_tune_mrr - pretrained_mrr\n",
        "b_a_standard = fine_tune_deviation - pretrained_deviation\n",
        "square_rt_n = math.sqrt(len(pretrained_mrr_list))\n",
        "print(\"B - A: \" + str(b_a))\n",
        "print(\"standard dev: \" + str(b_a_standard))\n",
        "print(\"Square root n: \" + str(square_rt_n))\n",
        "t = (b_a / b_a_standard) * square_rt_n\n",
        "print(\"t: \" + str(round(t, 4)))\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "interpreter": {
      "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
    },
    "kernelspec": {
      "display_name": "Python 3.10.9 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
